---
title: "Ecommerce"
author: "Melisa"
date: "19 July 2020"
output:
  pdf_document:
    fig_height: 20
    fig_width: 20
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Business understanding
Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. Kira Plastinina, the world’s youngest fashion designer who opened her first boutique when she was just 14, is one of the most disputed fashion figures in Russia. Fans of her clothing argue that she’s brought a breath of fresh air to the industry 

# Business objectives
 The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.

# Assessing the situation at hand
## Resources inventory
### Datasets 

Dataset url<-<- http://bit.ly/EcommerceCustomersDataset

Software (Github,R)

# Assumptions 
The data is up to date and relevant.

# Dataset information
* The dataset consists of 10 numerical and 8 categorical attributes. The 'Revenue' attribute can be used as the class label.

* "Administrative", "Administrative Duration", "Informational", "Informational Duration", "Product Related" and "Product Related Duration" represents the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real-time when a user takes an action, e.g. moving from one page to another. 

* The "Bounce Rate", "Exit Rate" and "Page Value" features represent the metrics measured by "Google Analytics" for each page in the e-commerce site. 

* The value of the "Bounce Rate" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session.

* The value of the "Exit Rate" feature for a specific web page is calculated as for all pageviews to the page, the percentage that was the last in the session.

* The "Page Value" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. 

* The "Special Day" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with the transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. 

* The dataset also includes the operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.

# Constraints
There are no constraints currently

# Load necessary libraries
```{r}
library(tidyverse)# Enables us to carry out data cleaning
library(DataExplorer)# Enables us to carry our EDA
library(ggcorrplot)# Enables us to plot a correlation matrix
library(corrplot)
library(skimr)# Enables us to draw more information on our dataset
```



# Load in dataset
```{r}
ecommerce <-read_csv("online_shoppers_intention.csv")
view(ecommerce)
```


# Preview of head of our dataset
```{r}
head(ecommerce)
```


# Preview of the tail of our dataset
```{r}
tail(ecommerce)
```


# Summary of our dataset's information
```{r}
names(ecommerce)
```


# Summary of our dataset
```{r}
summary(ecommerce)
```

```{r}
skim(ecommerce)
```

# dimension of the dataset
```{r}
dim(ecommerce)
```

The dataset has 12330 observations of 18 variables


# Tidying our dataset


## Check for duplicates
```{r}
# duplicated rows in the ecommerce df 
# and assign to a variable duplicated_rows

duplicated_rows <- ecommerce[duplicated(ecommerce),]

# Lets print out the variable duplicated_rows and see these duplicated rows 

dim(duplicated_rows)
```

```{r}
# pick the non-duplicated rows
ecommerce = ecommerce[!duplicated(ecommerce), ]
dim(ecommerce)
```


## Check for null values 
```{r}
colSums(is.na(ecommerce))

```



## Remove missing values

```{r}
edf <- na.omit(ecommerce) 
```



## Confirm the values have been dropped
```{r}
colSums(is.na(edf))
```


## Outliers
### Subset your data to numerical columns only
```{r}
num <- edf[, c(1,2,3,4,5,6,7,8,9,10)]
```



```{r}
names(num)
```

```{r}
boxplot(num$Administrative,col = "red",xlab ='Administrative', boxwex=0.2)
boxplot(num$Administrative_Duration,col = "blue",xlab = 'Administrative_Duration')
boxplot(num$Informational,col = "sienna",xlab = 'Informational')
boxplot(num$Informational_Duration,col = "yellow",xlab = 'Informational_Duration')
boxplot(num$ProductRelated,col = "maroon",xlab = 'ProductRelated')
boxplot(num$ProductRelated_Duration,col = "purple",xlab = 'ProductRelated_Duration')
boxplot(num$BounceRates,col = "green",xlab = 'BounceRates')
boxplot(num$ExitRates,col = "yellow",xlab = 'ExitRates')
boxplot(num$PageValues,col = "cyan",xlab = 'PageValues')
boxplot(num$SpecialDay,col = "pink",xlab = 'SpecialDay')

```


# Univariate analysis

## Frequency tables
```{r}
table(ecommerce$VisitorType)
```

From the analysis above we can see that they had many returning visitors



```{r}
table(ecommerce$Month)
```

We can also see that May was the month with the most visits

```{r}

table(ecommerce$TrafficType)
```




## Histogram
```{r}
plot_histogram(edf)

```
Most of the data is skewed to the left


## Distribution 
```{r}
plot_bar(edf)
```


# Bivariate analysis


```{r}
cor(edf$Administrative ,edf$Administrative_Duration)
cor(edf$Informational,edf$Informational_Duration)
cor(edf$ProductRelated,edf$ProductRelated_Duration)
```

```{r}
library(ggplot2)

j = ggplot(data = edf, aes(x = Administrative , fill = Revenue))+ 
geom_bar(width = 0.5)
j
```

Most administrative counts have false revenues than true ones




# Multivariate analysis
### Correlation

```{r}
num <- edf[, c(1,2,3,4,5,6,7,8,9,10)]
```

```{r}
data.cor = cor(num)
corrplot(data.cor, type = 'upper')
```

### Pairplot
```{r}
pairs(edf[,c(1,2,3,4,6,7,8,9,10)])
```

### Correlation matrix
```{r}
options(repr.plot.width=20,repr.plot.height = 20)
corr = round(cor(select_if(edf, is.numeric)), 1)
ggcorrplot(corr, hc.order = T, ggtheme = ggplot2::theme_grey,
           colors = c("cyan", "blue", "maroon"), lab = T)

```


## Feature importance
```{r}
data.pca <- prcomp(edf[,c(1,2,3,4,5,6,7,8,9,10)], center = TRUE,scale. = TRUE)

summary(data.pca)
```


```{r}
plot_prcomp(num, variance_cap = 0.9, nrow = 2L, ncol = 2L)

```


# K means 


# Creating new dataframe

```{r}
edf.new <- edf[,c(1:17)]
```

```{r}
edf.class <- edf["Revenue"]
```

## Normalize the data
```{r}
normalize <- function(x){
  return ((x-min(x)) / (max(x)-min(x)))
}
edf.new$Administrative <-normalize(edf.new$Administrative)
edf.new$Administrative_Duration <-normalize(edf.new$Administrative_Duration)
edf.new$Informational <-normalize(edf.new$Informational)
edf.new$Informational_Duration <-normalize(edf.new$Informational_Duration)
edf.new$ProductRelated <-normalize(edf.new$ProductRelated)
edf.new$ProductRelated_Duration <-normalize(edf.new$ProductRelated_Duration)
edf.new$BounceRates <-normalize(edf.new$BounceRates)
edf.new$ExitRates <-normalize(edf.new$ExitRates)
edf.new$PageValues <-normalize(edf.new$PageValues)
edf.new$SpecialDay <-normalize(edf.new$SpecialDay)
edf.new$OperatingSystems <-normalize(edf.new$OperatingSystems)
edf.new$Browser <-normalize(edf.new$Browser)
edf.new$Region<-normalize(edf.new$Region)
edf.new$TrafficType <-normalize(edf.new$TrafficType)
```

## Storing numeric data
```{r}
edf.clust <- edf.new[,c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay","OperatingSystems","Browser","Region","TrafficType")] 
```

## Applying K-mean clustering
```{r}
results<-kmeans(edf.clust,3)
```

## Preview the no of records in each cluster
```{r}
results$size
```

## Cluster centers
```{r}
results$centers
```

## Plot dist in clusters
```{r}
plot(edf.clust[c(1,2)],col = results$cluster)
```


```{r}
plot(edf.clust[c(3,4)],col = results$cluster)
```



```{r}
plot(edf.clust[c(5,6)],col = results$cluster)
```

## convert edf.clss to numeric
```{r}
edf.class[] <-lapply(edf.class,as.numeric)
sapply(edf.class,class)
```

## Accuracy
```{r}
mean(edf.new==results$cluster)
```

# Hierachical clustering

## Descriptive stats of edf.clust
```{r}
summary(round(edf.clust,1))
```

## scaling our data
Transforms our data to have a mean of 0 and std of 1
```{r}
edf.clust <- scale(edf.clust)
head(edf.clust)
```

# defining the distance that we will use
```{r}
d <-dist(edf.clust,method = "euclidian")
```

## clustering using hclust() fx
```{r}
edf.hc <- hclust (d,method = "ward.D2")
```

## Dendogram plot
```{r}
plot(edf.hc,cex = 0.6,hang = -1)
```

## cut the tree into three
```{r}
sub_group = cutree(edf.hc,k=3)
table(sub_group)
```





